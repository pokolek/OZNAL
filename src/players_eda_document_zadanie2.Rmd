
---
title: "Exploratory Data Analysis of Soccer Players Dataset"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(corrplot)
library(ggplot2)
library(e1071)
library(randomForest)
library(class)
library(pROC)
library(boot)
library(glmnet)
library(car)
```

## Load and Inspect the Dataset

```{r load-inspect}
players <- read.csv("C:/Users/marti/OneDrive/Documents/OZNAL/OZNAL/data/players_22.csv")
#players <- read.csv("C:/Users/olko/Desktop/OZNAL/data/players_22.csv")
glimpse(players) # Provides a brief overview of the dataset
summary(players) # Summarizes the data giving quick insights
```

## Data Cleaning

Remove players, that play as a goalkeeper

```{r GK}
players <- players %>% 
  filter(!str_detect(player_positions, "GK")) 
```

Visualize relationships between variables 
```{r}
# Selecting specific columns for the pair plot
selected_columns <- players[, c("value_eur", "potential", "overall", "age", "wage_eur", "height_cm", "weight_kg", "league_level", "club_contract_valid_until", "nation_team_id", "release_clause_eur", "skill_moves", "pace", "shooting", "passing", "dribbling", "defending", "physic")]

# Removing rows with NA values to ensure the plot works correctly
selected_columns_clean <- na.omit(selected_columns)

# Plot value_eur vs potential
ggplot(selected_columns_clean, aes(x = potential, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. potential", x = "Potential", y = "Market Value (EUR)")

# Plot value_eur vs overall
ggplot(selected_columns_clean, aes(x = overall, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. overall", x = "Overall Rating", y = "Market Value (EUR)")

# Plot value_eur vs age
ggplot(selected_columns_clean, aes(x = age, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. age", x = "Age", y = "Market Value (EUR)")

# Plot value_eur vs wage_eur
ggplot(selected_columns_clean, aes(x = wage_eur, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. wage_eur", x = "wage_eur", y = "Market Value (EUR)")

# Plot value_eur vs height_cm
ggplot(selected_columns_clean, aes(x = height_cm, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. height_cm", x = "height_cm", y = "Market Value (EUR)")

# Plot value_eur vs weight_kg
ggplot(selected_columns_clean, aes(x = weight_kg, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. weight_kg", x = "weight_kg", y = "Market Value (EUR)")

# Plot value_eur vs league_level
ggplot(selected_columns_clean, aes(x = league_level, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. league_level", x = "league_level", y = "Market Value (EUR)")

# Plot value_eur vs club_contract_valid_until
ggplot(selected_columns_clean, aes(x = club_contract_valid_until, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. club_contract_valid_until", x = "club_contract_valid_until", y = "Market Value (EUR)")

# Plot value_eur vs nation_team_id
ggplot(selected_columns_clean, aes(x = nation_team_id, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. nation_team_id", x = "nation_team_id", y = "Market Value (EUR)")

# Plot value_eur vs release_clause_eur
ggplot(selected_columns_clean, aes(x = release_clause_eur, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. release_clause_eur", x = "release_clause_eur", y = "Market Value (EUR)")

# Plot value_eur vs skill_moves
ggplot(selected_columns_clean, aes(x = skill_moves, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. skill_moves", x = "skill_moves", y = "Market Value (EUR)")

# Plot value_eur vs pace
ggplot(selected_columns_clean, aes(x = pace, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. pace", x = "pace", y = "Market Value (EUR)")

# Plot value_eur vs shooting
ggplot(selected_columns_clean, aes(x = shooting, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. shooting", x = "shooting", y = "Market Value (EUR)")

# Plot value_eur vs passing
ggplot(selected_columns_clean, aes(x = passing, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. passing", x = "passing", y = "Market Value (EUR)")

# Plot value_eur vs dribbling
ggplot(selected_columns_clean, aes(x = dribbling, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. dribbling", x = "dribbling", y = "Market Value (EUR)")

# Plot value_eur vs defending
ggplot(selected_columns_clean, aes(x = defending, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. defending", x = "defending", y = "Market Value (EUR)")

# Plot value_eur vs physic
ggplot(selected_columns_clean, aes(x = physic, y = value_eur)) + 
  geom_point() + 
  theme_minimal() + 
  labs(title = "value_eur vs. physic", x = "physic", y = "Market Value (EUR)")
```

```{r Histograms for Skill Attributes by Position}
players_simplified <- players %>%
  mutate(primary_position = str_split(player_positions, ",", simplify = TRUE)[,1])

# Histograms for Skill Attributes by Primary Position
players_simplified %>%
  gather(key="skill", value="value", pace, shooting, passing, dribbling, defending, physic, weight_kg, height_cm) %>%
  mutate(skill = factor(skill)) %>%
  ggplot(aes(x=value, fill=primary_position)) + 
  geom_histogram(bins=20, alpha=0.5, position="identity") + 
  facet_wrap(~skill, scales="free_x") +
  theme_minimal() +
  theme(legend.position="bottom") +
  labs(title="Distribution of Skill Attributes by Primary Player Positions", x="Attribute Value", y="Frequency")
```

Count sum of missing values
```{r data-cleaning}
count_na_in_column <- function(data, column_name) {
  na_count <- table(is.na(data[[column_name]]))
  print(na_count)
}

count_na_in_column(players, 'overall')
count_na_in_column(players, 'potential')
count_na_in_column(players, 'wage_eur')
count_na_in_column(players, 'value_eur')

count_na_in_column(players, 'player_positions')
count_na_in_column(players, 'pace')
count_na_in_column(players, 'shooting')
count_na_in_column(players, 'passing')
count_na_in_column(players, 'dribbling')
count_na_in_column(players, 'defending')
count_na_in_column(players, 'physic')
count_na_in_column(players, 'height_cm')
count_na_in_column(players, 'weight_kg')
```

Removing rows with missing values
```{r data-removing}
players <- players %>% 
  filter(!is.na(wage_eur))

players <- players %>% 
  filter(!is.na(value_eur))
```

Identifying outliers
```{r identifying-outliers}
#Boxplots to see if there are any outliers
boxplot(players$overall, players$potential, 
        names = c("Overall", "Potential"),
        main = "Overall and Potential")

boxplot(players$wage_eur,
        names = "Wage",
        main = "Wage")

boxplot(players$value_eur,
        names = "Value",
        main = "Value")

boxplot(players$pace, players$shooting, players$dribbling, players$defending, players$physic,
        names = c("pace", "shooting", "dribbling", "defending", "physic"),
        main = "Physical atributes")

boxplot(players$height_cm, players$weight_kg,
        names = c("height_cm", "weight_kg"),
        main = "Physical parameters")

```
Handling outliers
```{r Outlier-handling}
Q1 <- quantile(players$pace, 0.25)
Q3 <- quantile(players$pace, 0.75)
IQR <- Q3 - Q1
players <- subset(players, players$pace >= (Q1 - 1.5 * IQR) & players$pace <= (Q3 + 1.5 * IQR))

Q1 <- quantile(players$dribbling, 0.25)
Q3 <- quantile(players$dribbling, 0.75)
IQR <- Q3 - Q1
players <- subset(players, players$dribbling >= (Q1 - 1.5 * IQR) & players$dribbling <= (Q3 + 1.5 * IQR))

Q1 <- quantile(players$physic, 0.25)
Q3 <- quantile(players$physic, 0.75)
IQR <- Q3 - Q1
players <- subset(players, players$physic >= (Q1 - 1.5 * IQR) & players$physic <= (Q3 + 1.5 * IQR))

Q1 <- quantile(players$weight_kg, 0.25)
Q3 <- quantile(players$weight_kg, 0.75)
IQR <- Q3 - Q1
players <- subset(players, players$weight_kg >= (Q1 - 1.5 * IQR) & players$weight_kg <= (Q3 + 1.5 * IQR))

Q1 <- quantile(players$height_cm, 0.25)
Q3 <- quantile(players$height_cm, 0.75)
IQR <- Q3 - Q1
players <- subset(players, players$height_cm >= (Q1 - 1.5 * IQR) & players$height_cm <= (Q3 + 1.5 * IQR))

```

```{r Histograms for Skill Attributes by Position after handling outliers}
players_simplified <- players %>%
  mutate(primary_position = str_split(player_positions, ",", simplify = TRUE)[,1])

# Histograms for Skill Attributes by Primary Position
players_simplified %>%
  gather(key="skill", value="value", pace, shooting, passing, dribbling, defending, physic, weight_kg, height_cm) %>%
  mutate(skill = factor(skill)) %>%
  ggplot(aes(x=value, fill=primary_position)) + 
  geom_histogram(bins=20, alpha=0.5, position="identity") + 
  facet_wrap(~skill, scales="free_x") +
  theme_minimal() +
  theme(legend.position="bottom") +
  labs(title="Distribution of Skill Attributes by Primary Player Positions", x="Attribute Value", y="Frequency")
```

```{r Prepare Data for Classification}
# "B" stands for defenders like "CB", "LB", "RB"
players$IsDefender <- ifelse(grepl("B", players$player_positions), 1, 0) 

#
predictors <- c("pace", "shooting", "passing", "dribbling", "defending", "physic", "height_cm", "weight_kg")

# Prepare the dataset (ensure that you have no missing values in the predictors and the target variable)
classification_data <- players %>% 
  select(IsDefender, all_of(predictors)) %>% 
  na.omit()

# Create training and testing sets
set.seed(123) # for reproducibility
training_rows <- createDataPartition(classification_data$IsDefender, p = 0.8, list = FALSE)
train_data <- classification_data[training_rows, ]
test_data <- classification_data[-training_rows, ]

```

```{r feature selection}
continuous_vars <- c("pace", "shooting", "passing", "dribbling", "defending", "physic", "height_cm", "weight_kg")
for(var in continuous_vars) {
  p <- ggplot(train_data, aes(x = .data[[var]], y = IsDefender)) +
    geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
    ggtitle(paste("Log-Odds Linearity Check for", var)) +
    xlab(var) +
    ylab("Probability of Being a Defender")
  print(p)
}

```

## Log-Odds Linearity Plots

These plots are checking the assumption of linearity between each predictor variable and the log-odds of the dependent variable, which is a key assumption in logistic regression. The assumption is not about linearity in the probability scale (which these plots are in), but rather in the log-odds scale. However, if the relationship looks reasonably straight on these plots, it often suggests that the relationship in the log-odds scale is also linear.

**Pace**: The plot indicates a fairly linear negative relationship between pace and the probability of being a defender. Faster players (higher pace) tend to be less likely defenders.

**Shooting**: The relationship curve starts flat at higher shooting values and then steepens as the shooting value decreases. This suggests players with lower shooting scores are much more likely to be defenders.

**Passing, Dribbling, Defending, Physic, Height_cm, Weight_kg**: Each of these plots shows a non-linear relationship. For Defending and Physic, there's a steep increase in the probability of being a defender with higher scores, which intuitively makes sense. For Height and Weight, it also increases, suggesting taller and heavier players are more likely to be defenders. The non-linearity in these plots suggests that including these variables as linear terms might not fully capture the relationship between them and the log-odds of being a defender.

## Variance Inflation Factor (VIF)
VIF values provide a measure of how much the variance of the estimated regression coefficients is inflated due to multicollinearity among the predictor variables.

- **VIF Value < 5**: Generally, VIF values less than 5 indicate that multicollinearity is not severe. In your output, pace, shooting, defending, physic, height_cm, and weight_kg all have VIF values below this threshold, suggesting that they do not have concerning levels of multicollinearity.
- **VIF Value between 5 and 10**: This range indicates moderate multicollinearity. It may be a concern, but not necessarily one that will impair the model. passing and dribbling fall into this category, indicating that they have some linear association with other variables in the model but not enough to be overly problematic.
- **VIF Value > 10**: Values greater than 10 are typically considered to be indicative of high multicollinearity, requiring further attention, such as removing the variable, combining it with another variable, or using dimensionality reduction techniques.
```{r VIF}
# Calculate VIF
vif_model <- vif(glm(IsDefender ~ ., data = train_data, family = "binomial"))
print(vif_model)
```

In summary, the plots show that some of the predictor variables have a non-linear relationship with the probability of being a defender. This might require transforming these variables or including them in a non-linear fashion in the logistic regression model. The VIF results mostly indicate acceptable levels of multicollinearity, except for passing and dribbling, which show moderate multicollinearity and might warrant closer inspection or remedial measures.


## Cross-Validation Plots Interpretation

The plots show how the mean squared error (MSE) from cross-validation varies with different values of the regularization parameter, log(λ). The MSE is lowest at the optimal λ value, which is indicated by the vertical dashed line. The leftmost vertical line in the plots represents lambda.min, the value of λ that minimizes the cross-validation error. For Lasso regularization, it's common to also consider lambda.1se (not shown), which is a more regularized model within one standard error of the minimum MSE.

The number above the plot represents the number of predictors that are included in the model at each λ value. As λ increases (left to right on the x-axis), the number of predictors in the model decreases because larger λ values impose a greater penalty on the coefficients, leading to more coefficients being shrunk towards zero.



## Coefficients from Elastic Net Model

Here's how to interpret the coefficients from the Elastic Net regularization (alpha = 0.5):

- **(Intercept)**: The baseline log odds of being a defender when all other variables are zero is negative, suggesting a low baseline probability of a player being a defender in the absence of other information.
- **pace**: A positive coefficient suggests a positive relationship with the outcome; as pace increases, the log odds of being a defender slightly increase.
- **shooting**: A negative coefficient implies that as shooting ability increases, the likelihood of being a defender decreases.
- **passing**: Similarly, a negative coefficient for passing suggests that players with better passing skills are less likely to be defenders.
- **dribbling**: A negative coefficient, although very close to zero, which may suggest a weak or negligible relationship with being a defender.
defending: A positive coefficient, and it's the largest in magnitude, which indicates a strong positive association with being a defender.
- **physic**: A small positive coefficient means that a better physical score is slightly associated with being a defender.
- **height_cm**: The positive coefficient indicates that taller players are slightly more likely to be defenders.
- **weight_kg**: A positive coefficient suggests heavier players are more likely to be defenders.

```{r Ridge}

# Prepare data
x <- model.matrix(IsDefender ~ . - 1, data = train_data)
y <- train_data$IsDefender

#fit elastic net model
cv_glmnet <- cv.glmnet(x, y, alpha = 0.5)  # alpha=1 for Lasso, alpha=0 for Ridge
plot(cv_glmnet)

# Get coefficients at the optimal lambda
best_lambda <- cv_glmnet$lambda.min
coef(cv_glmnet, s = best_lambda)
```


## Coefficients from Lasso Model

The Lasso model (alpha = 1) coefficients are quite similar to those from the Elastic Net model. The biggest difference with Lasso is that it can set some coefficients exactly to zero, which isn't happening with the variables shown here.

In both models, the defending attribute has the strongest positive association with being a defender, which is consistent with what you would expect in a soccer dataset: players with higher defending skills are more likely to be classified as defenders. Negative coefficients for shooting, passing, and dribbling suggest that players with higher abilities in these areas are less likely to be defenders, possibly because these skills are more critical for forwards and midfielders.


```{r lasso}
# Fit Lasso model
cv_glmnet <- cv.glmnet(x, y, alpha = 1)  # Set alpha to 1 for Lasso
plot(cv_glmnet)

# Extract coefficients at the optimal lambda
best_lambda <- cv_glmnet$lambda.min
lasso_coefs <- coef(cv_glmnet, s = best_lambda)
print(lasso_coefs)
```


1. **Defending Skills**: The `defending` attribute has the most substantial positive association with being a defender across both models. This finding is intuitive and confirms that this feature is likely the most important predictor for determining if a player is a defender.

2. **Offensive Skills**: Attributes typically associated with offensive skills, such as `shooting`, `passing`, and `dribbling`, are negatively associated with the likelihood of being a defender, which also aligns with domain knowledge of soccer.

3. **Physical Attributes**: `height_cm` and `weight_kg` have positive associations with being a defender, suggesting that larger physical size is a characteristic of defensive players in dataset.

4. **Sparse Model**: The Lasso model did not set any coefficients to zero, which means it did not exclude any predictors from the model based on the provided output.

5. **Model Selection**: The Elastic Net model combines the feature selection properties of Lasso with the shrinkage of Ridge. The blend has moderated the coefficients, and because no coefficients are zero, it suggests that all the features included might have some relationship with the outcome.


```{r Logistic Regression}
# Logistic Regression Model
logit_model <- glm(IsDefender ~ ., data = train_data, family = "binomial")
logit_predictions <- predict(logit_model, newdata = test_data, type = "response")
logit_predicted_classes <- ifelse(logit_predictions > 0.5, 1, 0)
# Convert both predicted classes and actual classes to factors with the same levels
logit_predicted_classes <- factor(logit_predicted_classes, levels = c(0, 1))
test_data$IsDefender <- factor(test_data$IsDefender, levels = c(0, 1))
# Create confusion matrix
logit_confMatrix <- confusionMatrix(logit_predicted_classes, test_data$IsDefender)
print(logit_confMatrix)
```

The confusion matrix and associated statistics from the logistic regression model output tell us how well the model performed on the test dataset. Let’s interpret the results:

### Confusion Matrix
- **True Negatives (TN)**: 1788 non-defenders were correctly predicted as non-defenders.
- **False Positives (FP)**: 165 non-defenders were incorrectly predicted as defenders.
- **False Negatives (FN)**: 192 defenders were incorrectly predicted as non-defenders.
- **True Positives (TP)**: 1263 defenders were correctly predicted as defenders.

### Model Performance Metrics
- **Accuracy**: Approximately 89.52% of the predictions were correct. The confidence interval (CI) for the accuracy is between 88.45% and 90.53%, which provides a range within which the true accuracy is likely to lie.
- **No Information Rate (NIR)**: This is the accuracy that could be achieved by always predicting the most frequent class. In this case, it's 58.1%. Since the model's accuracy is significantly higher than the NIR (p-value < 2e-16), we can conclude that the model is useful.
- **Kappa**: The Kappa statistic is a measure of how much better the classifier is compared to a classifier that makes predictions randomly, accounting for chance agreement. A Kappa of 0.7854 suggests a substantial agreement beyond chance.
- **Mcnemar's Test**: This test checks the model's performance on the two types of errors (FP and FN). A p-value of 0.1688 indicates there is no significant difference in the FP and FN rates; the model does not seem to be systematically biased towards one class.
- **Sensitivity** (Recall or True Positive Rate): About 90.30% of actual defenders were correctly identified, suggesting the model is quite good at detecting the positive class.
- **Specificity** (True Negative Rate): About 88.45% of actual non-defenders were correctly identified, which is also a good rate.
- **Positive Predictive Value** (Precision): When the model predicts a player is a defender, it is correct 91.55% of the time.
- **Negative Predictive Value**: When the model predicts a player is not a defender, it is correct 86.80% of the time.
- **Prevalence**: This is the actual occurrence rate of defenders in the test dataset, which is 58.1%.
- **Detection Rate**: This is the rate of true positive predictions; about 52.46% of all players were correctly identified as defenders by the model.
- **Detection Prevalence**: This is the proportion of positive predictions; about 57.31% of all predictions were that players are defenders.
- **Balanced Accuracy**: It averages sensitivity and specificity, and at 89.37%, it indicates the model is equally good at identifying both classes.

### Conclusion
The logistic regression model shows high accuracy and a strong ability to discriminate between defenders and non-defenders. The Kappa statistic indicates a substantial agreement, and the model's balance between sensitivity and specificity is good. The accuracy is much higher than the No Information Rate, which suggests that the model has predictive power beyond random guessing.

However, we should consider whether the classes are balanced. If they are not, accuracy may not be the best metric. In this case, the positive class prevalence is about 58%, indicating a somewhat balanced dataset, so accuracy is informative. Nonetheless, we should still consider both precision (positive predictive value) and recall (sensitivity) in the context of the problem.

Overall, the model seems to perform well, but the final assessment should consider the specific cost of false positives versus false negatives in the practical application where the model will be used.

```{r}
players$IsDefender <- ifelse(grepl("B", players$player_positions), 1, 0) 

# Exclude offensive features: 'pace', 'shooting', 'passing', 'dribbling'
predictors <- c("defending", "physic", "height_cm", "weight_kg")

# Prepare the dataset (ensure that you have no missing values in the predictors and the target variable)
classification_data <- players %>% 
  select(IsDefender, all_of(predictors)) %>% 
  na.omit()

# Create reduced training and testing sets
set.seed(123) # for reproducibility
training_rows <- createDataPartition(classification_data$IsDefender, p = 0.8, list = FALSE)
reduced_train_data <- classification_data[training_rows, ]
reduced_test_data <- classification_data[-training_rows, ]
```

```{r}
# Logistic Regression Model with reduced predictors
logit_model <- glm(IsDefender ~ ., data = reduced_train_data, family = "binomial")
logit_predictions <- predict(logit_model, newdata = reduced_test_data, type = "response")
logit_predicted_classes <- ifelse(logit_predictions > 0.5, 1, 0)
# Convert both predicted classes and actual classes to factors with the same levels
logit_predicted_classes <- factor(logit_predicted_classes, levels = c(0, 1))
reduced_test_data$IsDefender <- factor(reduced_test_data$IsDefender, levels = c(0, 1))
# Create confusion matrix
logit_confMatrix <- confusionMatrix(logit_predicted_classes, reduced_test_data$IsDefender)
print(logit_confMatrix)
```

Comparing the confusion matrix of the reduced model to the previous full model, we can observe the following differences:

### Confusion Matrix Comparison
- The reduced model has more false negatives (450 vs. 192) and more false positives (295 vs. 165) than the full model. This suggests that the reduced model, which excludes offensive features, is less capable of correctly classifying defenders and non-defenders.
- The reduced model has fewer true positives (1133 vs. 1263) and true negatives (1530 vs. 1788), indicating a decrease in its ability to correctly identify each class.

### Model Performance Metrics
- **Accuracy**: The reduced model's accuracy is approximately 78.14%, which is lower than the full model's 89.52% accuracy. This suggests that excluding offensive features has led to a loss of predictive power.
- **Kappa**: The Kappa statistic has decreased from 0.7854 to 0.5577, which still indicates a moderate agreement beyond chance, but it's lower than the full model, implying a diminished predictive quality.
- **Mcnemar's Test**: The p-value is significantly low (1.68e-08), indicating a significant difference between the false positive and false negative rates. This could suggest that the model is more prone to one type of error over the other.
- **Sensitivity**: Has decreased slightly (90.30% vs. 77.27%).
- **Specificity**: Has decreased slightly (88.45% vs. 79.34%).
- **Positive Predictive Value**: Has decreased (91.55% vs. 83.84%).
- **Negative Predictive Value**: Has decreased significantly (86.80% vs. 71.57%).
- **Balanced Accuracy**: Has decreased from 89.37% to 78.31%.

### Conclusion
The removal of offensive features from the model has resulted in a noticeable decrease in all performance metrics. The most significant reductions are in negative predictive value and accuracy. This indicates that offensive attributes such as 'pace', 'shooting', 'passing', and 'dribbling' contribute valuable information for predicting whether a player is a defender and that their exclusion impairs the model's predictive ability.

From these observations, we can conclude that while simplifying the model can have benefits, such as improved interpretability and potentially reduced overfitting, it is important to carefully consider which features to exclude. Features that may seem less important could still be providing essential information for the model, and their exclusion can negatively impact performance.

The results demonstrate the importance of including a comprehensive set of features that capture the full spectrum of influences on the target variable. For a nuanced decision like determining a player's position, which can be influenced by a variety of factors, a more complex model with a richer feature set appears to provide better performance.

```{r Logistic Regression Full Model on Train Data}
# Fit Logistic Regression Model on the full training data
full_logit_model_train <- glm(IsDefender ~ ., data = train_data, family = "binomial")

# Predict on the training data
train_predictions <- predict(full_logit_model_train, newdata = train_data, type = "response")
train_pred_classes <- ifelse(train_predictions > 0.5, 1, 0)

# Convert predictions and actual classes to factors
train_pred_classes <- factor(train_pred_classes, levels = c(0, 1))
train_data$IsDefender <- factor(train_data$IsDefender, levels = c(0, 1))

# Create confusion matrix for the training data
train_confMatrix <- confusionMatrix(train_pred_classes, train_data$IsDefender)
print(train_confMatrix)

```

```{r Logistic Regression Full Model on Test Data}
# Predict on the test data
test_predictions <- predict(full_logit_model_train, newdata = test_data, type = "response")
test_pred_classes <- ifelse(test_predictions > 0.5, 1, 0)

# Convert predictions and actual classes to factors
test_pred_classes <- factor(test_pred_classes, levels = c(0, 1))
test_data$IsDefender <- factor(test_data$IsDefender, levels = c(0, 1))

# Create confusion matrix for the test data
test_confMatrix <- confusionMatrix(test_pred_classes, test_data$IsDefender)
print(test_confMatrix)
```

```{r subser}
# Example: Analyze subset where 'defending' is above average
subset_data <- train_data[train_data$defending > mean(train_data$defending), ]
subset_predictions <- predict(full_logit_model_train, newdata = subset_data, type = "response")
subset_pred_classes <- ifelse(subset_predictions > 0.5, 1, 0)
subset_pred_classes <- factor(subset_pred_classes, levels = c(0, 1))
subset_data$IsDefender <- factor(subset_data$IsDefender, levels = c(0, 1))

# Confusion matrix for the subset
subset_confMatrix <- confusionMatrix(subset_pred_classes, subset_data$IsDefender)
print(subset_confMatrix)

```

From the results provided, you have several sets of confusion matrices and corresponding statistics for different scenarios: the full model's performance on the training data, on the test data, and on a subset of the training data where the 'defending' attribute is above average. Here’s how to interpret and compare these results:

### Performance on Training Data
- **Accuracy**: 89.47%, suggesting the model fits the training data well.
- **Sensitivity** and **Specificity**: High values (90.26% and 88.34%, respectively), indicating effective identification of both classes.
- **Positive Predictive Value (PPV)** and **Negative Predictive Value (NPV)**: High values (91.62% and 86.54%), indicating reliable predictions.
- **Kappa**: 0.7837, indicating substantial agreement.
- **Balanced Accuracy**: 89.30%, showing good balance between sensitivity and specificity.
- The **P-Value** from Mcnemar’s Test is significant, indicating an imbalance in the type of errors (more false negatives than false positives).

### Performance on Test Data
- **Accuracy**: Very similar to the training data at 89.52%, indicating that the model generalizes well to unseen data.
- **Sensitivity** and **Specificity**: Consistently high as in the training data, further confirming the model's robustness.
- **PPV** and **NPV**, **Kappa**, and **Balanced Accuracy**: Also similar to the training data results.
- The model maintains its predictive quality on new data, as shown by the similar statistics across the training and testing datasets.

### Performance on Subset of Training Data (Defending > Average)
- **Accuracy**: Slightly lower at 84.21% compared to the full training and testing datasets.
- **Sensitivity**: Significantly lower at 72.81%, indicating the model struggles somewhat to identify defenders correctly within this subset.
- **Specificity** and **PPV** remain high.
- **Balanced Accuracy**: Decreases to 81.40%, reflecting the drop in sensitivity.
- The **Kappa** value is also lower at 0.6398, indicating a moderate agreement beyond chance, which is a decrease compared to the full data results.

### Comparing Results and Conclusions
1. **Generalization**: The model generalizes well from training to test data, showing minimal overfitting as the accuracy and other metrics are very consistent across these datasets.
2. **Subset Analysis**: The model's performance drops in a subset of data where defending is above average. This could indicate that as defending values increase, other features might interact differently affecting the model's ability to generalize within this context.
3. **Practical Implications**: The lower performance in the subset could suggest the need for more complex modeling techniques that can handle higher interactions or non-linearities, particularly for players with higher defending skills.

Overall, the results suggest that your model is robust and performs well under general conditions but may require adjustments or more sophisticated methods to handle data subsets where player attributes significantly deviate from the norm. It also underscores the importance of examining model performance across different data segments to ensure consistent reliability.

```{r}
library(randomForest)
library(xgboost)
library(caret)

# Assuming 'train_data' is already prepared and contains the target 'IsDefender'
# and that 'train_data' and 'test_data' are already split.

# Fit Logistic Regression
logit_model <- glm(IsDefender ~ ., data = train_data, family = "binomial")

# Fit Random Forest
rf_model <- randomForest(IsDefender ~ ., data = train_data, importance = TRUE, ntree = 500)

# # Fit GBM
# control <- trainControl(method = "cv", number = 10)
# gbm_model <- train(IsDefender ~ ., data = train_data, method = "gbm", trControl = control, verbose = FALSE)

# Extract feature importance
logit_importance <- summary(logit_model)$coefficients
rf_importance <- importance(rf_model)
# gbm_importance <- varImp(gbm_model)$importance

# Print importances
print("Logistic Regression Coefficients:")
print(logit_importance)

print("Random Forest Feature Importance:")
print(rf_importance)
# 
# print("GBM Feature Importance:")
# print(gbm_importance)


```

```{r knn}

# Train k-Nearest Neighbors model
# Target variable is a factor for classification
train_data$IsDefender <- as.factor(train_data$IsDefender)

# Train k-Nearest Neighbors model with class probabilities
knn_fit <- knn3(x = train_data[, predictors, drop = FALSE], y = train_data$IsDefender, k = 5, prob = TRUE)

# Predict probabilities on the test set
# Note: Ensure that test_data contains the same predictors as train_data
knn_prob <- predict(knn_fit, newdata = test_data[, predictors, drop = FALSE], type = "prob")[,2]

# Convert probabilities to class predictions using a 0.5 threshold
knn_class_predictions <- ifelse(knn_prob > 0.5, "1", "0")

# Ensure the predictions are factors with the same levels as the test data target variable
knn_class_predictions <- factor(knn_class_predictions, levels = levels(train_data$IsDefender))

# Generate the confusion matrix
knn_confMatrix <- confusionMatrix(knn_class_predictions, test_data$IsDefender)

# Print the confusion matrix
print(knn_confMatrix)

```

```{r knn tuning}

# Prepare your data if not already done
# train_data, test_data

# Set up cross-validation
train_control <- trainControl(
  method = "cv", 
  number = 10,  # using 10-fold cross-validation
  savePredictions = "final"
)

# Define the tuning grid
k_values <- data.frame(k = seq(1, 20, by = 1))  # testing k from 1 to 20

# Train the model
set.seed(123)  # for reproducibility
knn_results <- train(
  x = train_data[, predictors, drop = FALSE], 
  y = train_data$IsDefender, 
  method = "knn", 
  tuneGrid = k_values,
  trControl = train_control,
  preProcess = "scale"  # scaling features
)

# Print the best model's results
print(knn_results)
plot(knn_results)
```


<!-- ```{r Naive Bayes Classifier} -->
<!-- # Naive Bayes Model -->
<!-- naive_model <- naiveBayes(IsDefender ~ ., data = train_data) -->
<!-- # Calculating probabilities for Naive Bayes Model -->
<!-- naive_probs <- predict(naive_model, newdata = test_data, type = "raw") -->
<!-- naive_predictions_probs <- naive_probs[,2] # Assuming class '1' probabilities are in the second column -->

<!-- # Predict class labels for Naive Bayes Model -->
<!-- naive_predictions <- predict(naive_model, newdata = test_data) -->
<!-- # Now you have naive_predictions for the confusionMatrix -->
<!-- naive_confMatrix <- confusionMatrix(naive_predictions, test_data$IsDefender) -->
<!-- print(naive_confMatrix) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # ROC and AUC for Logistic Regression -->
<!-- logit_roc <- roc(test_data$IsDefender ~ logit_predictions, data = test_data) -->
<!-- logit_auc <- auc(logit_roc) -->

<!-- # ROC and AUC for Naive Bayes - using the corrected probabilities -->
<!-- naive_roc <- roc(test_data$IsDefender ~ naive_predictions_probs, data = test_data) -->
<!-- naive_auc <- auc(naive_roc) -->

<!-- # ROC and AUC for kNN -->
<!-- knn_roc <- roc(response = test_data$IsDefender, predictor = knn_prob) -->
<!-- knn_auc <- auc(knn_roc) -->

<!-- # Plotting ROC Curves for all models -->
<!-- plot(logit_roc, col = "blue", main = "ROC Curves for Classification Models") -->
<!-- lines(naive_roc, col = "red") -->
<!-- lines(knn_roc, col = "green") -->
<!-- legend("bottomright",  -->
<!--        legend = c(sprintf("Logistic Regression (AUC = %.2f)", logit_auc), -->
<!--                   sprintf("Naive Bayes (AUC = %.2f)", naive_auc), -->
<!--                   sprintf("kNN (AUC = %.2f)", knn_auc)), -->
<!--        col = c("blue", "red", "green"), lwd = 2) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- # Compile AUC values into a summary table for all models -->
<!-- auc_summary_all <- tibble( -->
<!--   Model = c("Logistic Regression", "Naive Bayes", "kNN"), -->
<!--   AUC = c(logit_auc, naive_auc, knn_auc) -->
<!-- ) -->

<!-- # Display the AUC summary table for all models -->
<!-- knitr::kable(auc_summary_all, caption = "AUC Summary for All Classification Models") -->

<!-- ``` -->

## Model Evaluation

### Logistic Regression Evaluation

The Logistic Regression model achieved an accuracy of 89.52%. This model has demonstrated good predictive power, with a high Kappa statistic of 0.7854 indicating substantial agreement beyond chance. The model has a high sensitivity (90.30%) and specificity (88.45%), reflecting a balanced ability to identify both classes correctly.

### k-Nearest Neighbors (kNN) Evaluation

The kNN model, using a threshold of 0.5 to convert probabilities to class predictions, yielded an accuracy of 90.26%. This represents a slight improvement over the Logistic Regression model. The Kappa statistic of 0.799 suggests strong agreement. Sensitivity is higher at 92.83%, indicating that kNN is better at identifying defenders compared to the Logistic Regression model. The model also maintains good specificity at 86.69%.

### Naive Bayes Evaluation

The Naive Bayes model, while slightly less accurate at 84.45%, still performs reasonably well. It has a lower Kappa statistic of 0.6776, indicating moderate agreement. The sensitivity and specificity are 88.99% and 78.15%, respectively, suggesting that while it's good at identifying defenders, it has a higher rate of false positives compared to the other models.

### ROC Curve Comparison

The ROC curves for the three models illustrate their performance in classifying defenders. All three models show good performance with AUC values close to 1. The Logistic Regression model leads slightly with an AUC of 0.96, followed closely by kNN with an AUC of 0.95, and Naive Bayes with an AUC of 0.94.

## Conclusion

Based on the evaluation metrics, the k-Nearest Neighbors (kNN) model has the highest accuracy and a balanced accuracy rate, making it slightly superior in this specific case. However, Logistic Regression also shows robust performance and may be preferred for its interpretability. Naive Bayes, while not performing as strongly, is computationally efficient and could be favored for its speed in larger datasets or real-time applications.